"""
JAXç‰ˆæœ¬çš„æ•°æ®åŠ è½½å’ŒDirichletåˆ’åˆ†
æ³¨æ„ï¼šæ­¤æ¨¡å—ä¾èµ–å…¨å±€numpyéšæœºç§å­ï¼Œè°ƒç”¨å‰éœ€è®¾ç½®np.random.seed()
"""
import numpy as np
import jax.numpy as jnp
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from pathlib import Path


def get_dirichlet_data(y, n_clients, alpha, num_classes, verbose=False, balanced=True):
    """
    ä½¿ç”¨Dirichletåˆ†å¸ƒåˆ’åˆ†æ•°æ®
    
    å‚æ•°:
        y: æ ‡ç­¾æ•°ç»„
        n_clients: å®¢æˆ·ç«¯æ•°é‡
        alpha: Dirichletå‚æ•°
        num_classes: ç±»åˆ«æ•°é‡
        verbose: æ˜¯å¦æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        balanced: æ˜¯å¦ä½¿ç”¨balancedç‰ˆæœ¬ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯å›ºå®šæ ·æœ¬æ•°ï¼Œä¸FedRCLå¯¹é½ï¼‰
    
    è¿”å›:
        idx_batch: æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•
        net_cls_counts: æ¯ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«ç»Ÿè®¡
    """
    N = len(y)
    net_dataidx_map = {}
    
    if balanced:
        # Balancedç‰ˆæœ¬ï¼šæ¯ä¸ªå®¢æˆ·ç«¯å›ºå®šæ ·æœ¬æ•°ï¼ˆä¸FedRCLçš„cifar_dirichlet_balancedå¯¹é½ï¼‰
        num_data_per_client = int(N / n_clients)
        idx_batch = [[] for _ in range(n_clients)]
        assigned_ids = []
        
        for i in range(n_clients):
            # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯é‡‡æ ·ç±»åˆ«åˆ†å¸ƒ
            proportions = np.random.dirichlet(np.repeat(alpha, num_classes))
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆåŸºäºç±»åˆ«åˆ†å¸ƒï¼‰
            weights = np.zeros(N)
            for k in range(num_classes):
                idx_k = np.where(y == k)[0]
                weights[idx_k] = proportions[k]
            
            # å·²åˆ†é…çš„æ ·æœ¬æƒé‡è®¾ä¸º0
            weights[assigned_ids] = 0.0
            
            # å½’ä¸€åŒ–æƒé‡
            if weights.sum() > 0:
                weights = weights / weights.sum()
            else:
                # å¦‚æœæ‰€æœ‰æƒé‡ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                weights = np.ones(N)
                weights[assigned_ids] = 0.0
                weights = weights / weights.sum()
            
            # ä½¿ç”¨multinomialé‡‡æ ·å›ºå®šæ•°é‡çš„æ ·æœ¬
            # ç”±äºnumpyçš„multinomialä¸æ”¯æŒreplacement=Falseï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ æƒéšæœºé‡‡æ ·
            available_indices = np.where(weights > 0)[0]
            if len(available_indices) >= num_data_per_client:
                # ä½¿ç”¨åŠ æƒéšæœºé‡‡æ ·
                selected_indices = np.random.choice(
                    available_indices,
                    size=num_data_per_client,
                    replace=False,
                    p=weights[available_indices] / weights[available_indices].sum()
                )
            else:
                # å¦‚æœå¯ç”¨æ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬
                selected_indices = available_indices
            
            idx_batch[i] = selected_indices.tolist()
            assigned_ids.extend(selected_indices.tolist())
        
        # æ‰“ä¹±æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®
        for j in range(n_clients):
            np.random.shuffle(idx_batch[j])
            net_dataidx_map[j] = idx_batch[j]
    else:
        # Unbalancedç‰ˆæœ¬ï¼šåŸå§‹å®ç°
        p_client = np.zeros((n_clients, num_classes))
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯é‡‡æ ·ç±»åˆ«åˆ†å¸ƒ
        for i in range(n_clients):
            p_client[i] = np.random.dirichlet(np.repeat(alpha, num_classes))
        
        idx_batch = [[] for _ in range(n_clients)]
        
        # æŒ‰ç±»åˆ«åˆ†é…æ•°æ®
        for k in range(num_classes):
            idx_k = np.where(y == k)[0]
            np.random.shuffle(idx_k)
            proportions = p_client[:, k]
            proportions = proportions / proportions.sum()
            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
            idx_batch = [idx_j + idx.tolist() 
                        for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]
        
        # æ‰“ä¹±å¹¶ç»Ÿè®¡
        for j in range(n_clients):
            np.random.shuffle(idx_batch[j])
            net_dataidx_map[j] = idx_batch[j]
    
    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    net_cls_counts = {}
    for net_i, dataidx in net_dataidx_map.items():
        unq, unq_cnt = np.unique(y[dataidx], return_counts=True)
        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}
        net_cls_counts[net_i] = tmp
    
    if verbose:
        print('Data statistics: %s' % str(net_cls_counts))
        if balanced:
            sizes = [len(net_dataidx_map[i]) for i in range(n_clients)]
            print(f'Balanced split: min={min(sizes)}, max={max(sizes)}, mean={np.mean(sizes):.1f}')
    
    return idx_batch, net_cls_counts


def load_cifar10_data(n_clients, alpha, verbose=False):
    """
    åŠ è½½CIFAR-10æ•°æ®å¹¶ä½¿ç”¨Dirichletåˆ’åˆ†
    
    è¿”å›:
        train_datasets: æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒæ•°æ®(numpy arrays)
        test_data: æµ‹è¯•æ•°æ®(numpy array)
        net_cls_counts: ç±»åˆ«ç»Ÿè®¡
    """
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.491, 0.482, 0.447], 
                           std=[0.247, 0.243, 0.262])
    ])
    
    # æ•°æ®æ ¹ç›®å½•ï¼šå›ºå®šä¸ºæœ¬æ–‡ä»¶çˆ¶ç›®å½•ä¸‹çš„ data/cifar10ï¼ˆä¸è„šæœ¬è¿è¡Œç›®å½•æ— å…³ï¼‰
    data_root = str((Path(__file__).resolve().parent / 'data' / 'cifar10'))
    
    # åŠ è½½æ•°æ®é›†ï¼ˆä¸ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®ï¼‰
    dataset_train_global = datasets.CIFAR10(
        data_root, train=True, download=False, transform=transform)
    dataset_test_global = datasets.CIFAR10(
        data_root, train=False, download=False, transform=transform)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    train_loader = DataLoader(dataset_train_global, batch_size=len(dataset_train_global))
    test_loader = DataLoader(dataset_test_global, batch_size=len(dataset_test_global))
    
    X_train, Y_train = next(iter(train_loader))
    X_test, Y_test = next(iter(test_loader))
    
    X_train = X_train.numpy().transpose(0, 2, 3, 1)  # NCHW -> NHWC
    Y_train = Y_train.numpy()
    X_test = X_test.numpy().transpose(0, 2, 3, 1)  # NCHW -> NHWC
    Y_test = Y_test.numpy()
    
    # Dirichletåˆ’åˆ†ï¼ˆä½¿ç”¨balancedç‰ˆæœ¬ï¼Œä¸FedRCLå¯¹é½ï¼‰
    # ğŸ”§ æ³¨æ„ï¼šè°ƒç”¨è€…åº”è¯¥åœ¨è°ƒç”¨å‰è®¾ç½®np.random.seed()ä»¥ç¡®ä¿å¯é‡å¤æ€§
    idx_batch, net_cls_counts = get_dirichlet_data(Y_train, n_clients, alpha, 10, verbose=verbose, balanced=True)
    
    # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®é›†
    train_datasets = []
    for i, indices in enumerate(idx_batch):
        x_client = X_train[indices]
        y_client = Y_train[indices]
        train_datasets.append((x_client, y_client))
        if verbose:
            print(f"Client {i} Training examples: {len(x_client)}")
    
    return train_datasets, (X_test, Y_test), net_cls_counts


def load_cifar100_data(n_clients, alpha, verbose=False):
    """
    åŠ è½½CIFAR-100æ•°æ®å¹¶ä½¿ç”¨Dirichletåˆ’åˆ†ï¼ŒåŒæ—¶åœ¨æœ¬åœ°ä¸å­˜åœ¨æ•°æ®æ—¶è‡ªåŠ¨ä¸‹è½½ã€‚
    
    è¿”å›:
        train_datasets: æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒæ•°æ®(numpy arrays)
        test_data: æµ‹è¯•æ•°æ®(numpy array)
        net_cls_counts: ç±»åˆ«ç»Ÿè®¡
    """
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],
                             std=[0.2675, 0.2565, 0.2761])
    ])
    
    # æ•°æ®æ ¹ç›®å½•ï¼šå›ºå®šä¸ºæœ¬æ–‡ä»¶çˆ¶ç›®å½•ä¸‹çš„ data/cifar100ï¼ˆä¸è„šæœ¬è¿è¡Œç›®å½•æ— å…³ï¼‰
    data_root = str((Path(__file__).resolve().parent / 'data' / 'cifar100'))
    
    # åŠ è½½æ•°æ®é›†ï¼ˆä¸ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®ï¼‰
    dataset_train_global = datasets.CIFAR100(
        data_root, train=True, download=False, transform=transform)
    dataset_test_global = datasets.CIFAR100(
        data_root, train=False, download=False, transform=transform)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    train_loader = DataLoader(dataset_train_global, batch_size=len(dataset_train_global))
    test_loader = DataLoader(dataset_test_global, batch_size=len(dataset_test_global))
    
    X_train, Y_train = next(iter(train_loader))
    X_test, Y_test = next(iter(test_loader))
    
    X_train = X_train.numpy().transpose(0, 2, 3, 1)  # NCHW -> NHWC
    Y_train = Y_train.numpy()
    X_test = X_test.numpy().transpose(0, 2, 3, 1)  # NCHW -> NHWC
    Y_test = Y_test.numpy()
    
    # Dirichletåˆ’åˆ†ï¼ˆä½¿ç”¨balancedç‰ˆæœ¬ï¼Œä¸FedRCLå¯¹é½ï¼‰
    # ğŸ”§ æ³¨æ„ï¼šè°ƒç”¨è€…åº”è¯¥åœ¨è°ƒç”¨å‰è®¾ç½®np.random.seed()ä»¥ç¡®ä¿å¯é‡å¤æ€§
    idx_batch, net_cls_counts = get_dirichlet_data(Y_train, n_clients, alpha, 100, verbose=verbose, balanced=True)
    
    # åˆ›å»ºå®¢æˆ·ç«¯æ•°æ®é›†
    train_datasets = []
    for i, indices in enumerate(idx_batch):
        x_client = X_train[indices]
        y_client = Y_train[indices]
        train_datasets.append((x_client, y_client))
        if verbose:
            print(f"Client {i} Training examples: {len(x_client)}")
    
    return train_datasets, (X_test, Y_test), net_cls_counts


